{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from econfig import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Complete\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication Complete\")\n",
    "except:\n",
    "    print(\"Authentication Unable to Complete\")\n",
    "    \n",
    "two_days = (dt.datetime.now() - dt.timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "one_day = (dt.datetime.now() - dt.timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# startDate = datetime.datetime(2014, 6, 1, 0, 0, 0)\n",
    "# endDate =   datetime.datetime(2015, 1, 1, 0, 0, 0)\n",
    "MAX_TWEETS = 5\n",
    "tweets = tweepy.Cursor(api.search, q='#election2020', rpp=100, since=two_days, until=one_day).items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 836\n"
     ]
    }
   ],
   "source": [
    "tweet_list = []\n",
    "for tweet in tweets:\n",
    "    tweet_list.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'dataisgreat'\n",
    "conn = psycopg2.connect(user = \"postgres\",\n",
    "                                  password = f\"{p}\",\n",
    "                                  host = \"127.0.0.1\",\n",
    "                                  port = \"3306\",\n",
    "                                  database = \"postgres\")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweet_list:\n",
    "    cursor.execute('INSERT INTO edf_tweets (data) VALUES (%s);', (json.dumps(tweet),))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\"Tweets\":tweet_list})\n",
    "# # df.to_csv(\"Tweets_List.csv\",sep=\"|\")\n",
    "# # from pyspark import SparkFiles\n",
    "# # df = spark.read.csv(\"Tweets_List.csv\", sep=\"|\", header=True)\n",
    "\n",
    "# tokened = Tokenizer(inputCol=\"Tweets\", outputCol=\"words\")\n",
    "# tokened_transformed = tokened.transform(df)\n",
    "# tokened_transformed.show()\n",
    "\n",
    "\n",
    "# remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "# removed_frame = remover.transform(tokened_transformed)\n",
    "# # removed_frame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(tweet) for tweet in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets = []\n",
    "for tweet in tokenized:\n",
    "    filtered_tweet = []\n",
    "    for word in tweet:\n",
    "        if word.lower() not in stopwords and word not in string.punctuation and emoji_pattern.sub(r'', word):\n",
    "            filtered_tweet.append(emoji_pattern.sub(r'', word.lower()))\n",
    "    filtered_tweets.append(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for tweet in filtered_tweets:\n",
    "    for word in tweet:\n",
    "        if word.lower() in word_count.keys():\n",
    "            word_count[word.lower()] += 1\n",
    "        else:\n",
    "            word_count[word.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'key':\"value\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'key' in my_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_count[\"'s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.keys().contains('t.co')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_urls = [key for key, value in word_count.items() if 't.co' in key.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in twitter_urls:\n",
    "    del word_count[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    # generate word cloud\n",
    "wc.generate_from_frequencies(word_count)\n",
    "  \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"wordcloud.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_count[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['healthcare', 'terrorism', 'gun','guns', 'education', 'economy', 'immigration', 'climate', \\\n",
    "         'wealth', 'distribution', 'abortion', 'federal' 'budget', 'taxes', 'racism', 'sexism', 'metoo', \\\n",
    "         'diplomacy', 'lgbqt','lgbtq', 'equality', 'trade', 'war', 'democrat', 'republican', 'socialist', \"communist\", \\\n",
    "         'fascist', 'foreign', 'inclusivity', 'borders', 'medicare','nationalist','medicaid','democratic',\\\n",
    "        'inequality','justice','legalize','illegal','drugs','prescription','women','womens','rights','universal',\\\n",
    "        'supreme','court','potus','scotus','judge','maga','kaga','jobs','voting','voter','voters','daca',\\\n",
    "        'citizen','election','incumbent','independent','nominee','pac','party','lobbyist','gerrymandering',\\\n",
    "        'russia', 'russian', 'china', 'chinese', 'coronavirus', 'iowa', 'nevada', 'supertuesday', 'debate', \\\n",
    "         'debates', 'delegates', 'delegate', 'cnn', 'fox', 'golf', 'impeach', 'impeachment', 'senate', 'congress',\\\n",
    "         'capital','feelthebern', 'peteforamerica', 'ourbestdaysstilllieahead', 'notmeus', 'wintheera', \\\n",
    "         'dreambigfighthard', 'ilikemike', 'math', 'promisesmadepromiseskept', 'jobsnotmobs', 'keepamericagreat',\\\n",
    "        'believe', 'beliefs', 'bullshit', 'christian', 'family', 'values', 'millennials', 'latino', 'latina', 'latinx',\\\n",
    "        'grassroots','balance','policy','poll','platform','muslim','isis','arab','arabic','ewarren','berniesanders',\\\n",
    "        'petebuttigieg','realdonaltrump','mikebloomberg','joebiden','criminal', 'vice', 'epidemic', 'wrong', 'right',\\\n",
    "         'left','greed', 'threat', 'affordable', 'reform', 'promise', 'progress', 'support', 'protect', \\\n",
    "         'crisis', 'hate', 'debt', 'safe', 'freedom', 'win', 'lose','deficit','stock','market','nasdeq','hack','hacking',\\\n",
    "        'sanctuary','dissent','divide','divided','ballot','bailout','primary','usa','united','states','america','fraud',\\\n",
    "        'black','african','american','africanamerica','blacklivesmatter','blm','campgain','finance','civic','duty',\\\n",
    "         'altright','nazi','nazis','neonazi','neonazis','wage','wages','income','base','gop','epstein','capitalist',\\\n",
    "         'capitalism','dreamer','dreamers','border','detention','bluelivesmatter','ivanka','donald','melania']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_counts = {key:value for key, value in word_count.items() if key.lower() in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "wc = WordCloud(background_color=\"black\", max_words=1000)\n",
    "    # generate word cloud\n",
    "wc.generate_from_frequencies(word_list_counts)\n",
    "  \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"word_list_cloud.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create Python code to clean stream for Word Cloud (see above)\n",
    "2. Count occurances of words, store in DB\n",
    "    * Create json object with dictionary of words and count\n",
    "    * Store in DB\n",
    "    * Grab new data\n",
    "    * Grab current json object\n",
    "    * Update word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python36964bitpythondataconda387ec7e747ec49fcae7849ccd8da23ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
